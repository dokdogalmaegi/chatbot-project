import ollama
import json
import os

MODEL_NAME = "exaone3.5:7.8b"
HISTORY_FILE = "conversation_history.json"


def load_history():
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            messages = json.load(f)
        print("ğŸ’¬ ëŒ€í™” ê¸°ë¡ ë¡œë“œ ì™„ë£Œ.")
    else:
        messages = [{
            "role": "system",
            "content": (
                "ë‹¹ì‹ ì˜ ì´ë¦„ì€ ì´ì œë¶€í„° 'CubeBot'ì…ë‹ˆë‹¤. "
                "ì§ˆë¬¸ì— ë‹µë³€í•  ë•ŒëŠ” í•­ìƒ 'CubeBotì…ë‹ˆë‹¤. ë°˜ê°‘ìŠµë‹ˆë‹¤.'ë¥¼ ì‹œì‘ìœ¼ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”. "
                "ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µí•˜ì„¸ìš”."
            )
        }]
    return messages


def save_history(messages):
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(messages, f, ensure_ascii=False, indent=2)


def generate_response(messages):
    response = ollama.chat(model=MODEL_NAME, messages=messages, stream=True)
    full_reply = ""
    for chunk in response:
        token = chunk["message"]["content"]
        full_reply += token
        print(token, end='', flush=True)
    print("\n")
    return full_reply
