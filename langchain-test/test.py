from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.runnables import Runnable
from langchain_ollama import OllamaLLM
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, BaseMessage
import json
import os

# ì €ì¥ ê²½ë¡œ
HISTORY_DIR = "history"
os.makedirs(HISTORY_DIR, exist_ok=True)

# í”„ë¡¬í”„íŠ¸ ì •ì˜ (ëŒ€í™” ì´ë ¥ ë°˜ì˜ì„ ìœ„í•œ Placeholder í¬í•¨)
template = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ CubeBotì…ë‹ˆë‹¤. ëª¨ë“  ì‘ë‹µì€ 'CubeBotì…ë‹ˆë‹¤. ë°˜ê°‘ìŠµë‹ˆë‹¤.'ë¡œ ì‹œì‘í•˜ë©°, ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µí•˜ì„¸ìš”."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
])

# ëª¨ë¸ ì •ì˜
llm = OllamaLLM(model="exaone3.5:7.8b", streaming=True)

# ì²´ì¸ ì •ì˜
chain: Runnable = template | llm

# ëŒ€í™” ê¸°ë¡ì„ ë©”ëª¨ë¦¬ì— ì €ì¥
store = {}

def load_chat_history_from_json(filename: str) -> list[BaseMessage]:
    messages = []
    if not os.path.exists(filename):
        return messages
    with open(filename, "r", encoding="utf-8") as f:
        data = json.load(f)
        for msg in data:
            if msg["type"] == "human":
                messages.append(HumanMessage(content=msg["content"]))
            elif msg["type"] == "ai":
                messages.append(AIMessage(content=msg["content"]))
            elif msg["type"] == "system":
                messages.append(SystemMessage(content=msg["content"]))
    return messages

def save_chat_history_to_json(chat_history: list[BaseMessage], filename: str):
    messages = []
    for msg in chat_history:
        messages.append({
            "type": msg.type,
            "content": msg.content
        })
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(messages, f, ensure_ascii=False, indent=2)

def get_session_history(session_id: str) -> InMemoryChatMessageHistory:
    if session_id not in store:
        history = InMemoryChatMessageHistory()
        filepath = os.path.join(HISTORY_DIR, f"{session_id}_history.json")
        for msg in load_chat_history_from_json(filepath):
            history.add_message(msg)
        store[session_id] = history
    return store[session_id]

# Memory ë¥¼ ê°€ì§„ ì²´ì¸ìœ¼ë¡œ ë³€í™˜
chat_chain = RunnableWithMessageHistory(
    chain,
    lambda session_id: get_session_history(session_id),
    input_messages_key="input",
    history_messages_key="chat_history",
)

# ë©”ì¸ ë£¨í”„
if __name__ == "__main__":
    session_id = "user-1"  # ì„ì‹œ ê³ ì • (ì‚¬ìš©ìë³„ë¡œ ë¶„ë¦¬ ê°€ëŠ¥)

    while True:
        user_input = input("ğŸ™‹ ì‚¬ìš©ì: ")

        if user_input.strip().lower() in ["exit", "quit", "ì¢…ë£Œ"]:
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤. ì•ˆë…•íˆ ê°€ì„¸ìš”!")

            history = get_session_history(session_id)
            save_chat_history_to_json(history.messages, os.path.join(HISTORY_DIR, f"{session_id}_history.json"))
            print("ëŒ€í™” ê¸°ë¡ì´ conversation_history-test.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

            break

        print("ğŸ¤– CubeBot:", end=' ', flush=True)
        for chunk in chat_chain.stream({"input": user_input}, config={"configurable": {"session_id": session_id}}):
            print(getattr(chunk, "content", str(chunk)), end="", flush=True)
        print()
